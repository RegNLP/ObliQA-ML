
# ObliQA-MultiPassage

This repository builds on the ObliQA questions and answers generated by the RegNLP team:  
> **Source:** https://github.com/RegNLP/ObliQADataset  

From that original ObliQA dataset, we **keep only the questions** that are linked to **multiple passages**, and save them here as:

```bash
ObliQA-MultiPassage/  
└── ObliQA_MultiPassage.json
```
---

## Table of Contents

1. [Overview](#overview)  
2. [Data](#data)  
3. [Installation & Prerequisites](#installation--prerequisites)  
4. [Validation Pipeline](#validation-pipeline)  
   - [Configuration](#configuration)  
   - [Running the Validator](#running-the-validator)  
5. [Statistics & Splitting](#statistics--splitting)  
6. [Directory Structure](#directory-structure)  
7. [Contributing](#contributing)  
8. [License](#license)  

---

## Overview

ObliQA-MultiPassage is a multi-passage extension of the original ObliQA dataset. It supports:

- **Passage-level validation** (Directly vs. Indirectly vs. Not Connected) via GPT-4  
- **Automatic filtering** to keep only valid multi-passage questions  
- **Dataset statistics** (number of passages per question)  
- **Train/Validation/Test splits** (70/15/15 by default)  

---

## Data

1. **Original ObliQA**  
   All questions and single-passage answers were generated by the RegNLP/ObliQADataset pipeline.

2. **Multi-Passage Extension**  
   We filter for questions that reference **at least two passages** and include **at least one** “Directly Connected” passage. The result is saved as:

`ObliQA-MultiPassage/ObliQA_MultiPassage.json`

---

## Installation & Prerequisites

1. Clone or download this folder into your Google Drive (for Colab) or local machine.  
2. Install Python 3.8+ and the following packages:

```bash
pip install openai pandas
```

Obtain an OpenAI API key and set it in your environment:
```bash
export OPENAI_API_KEY="sk-…"
```

## Validation Pipeline

This step labels each `(question, passage)` pair as **Directly Connected**, **Indirectly Connected**, or **Not Connected** using GPT-4.

### Configuration

Edit the top of `validate.py` (or the notebook cells) to set:
```bash
INPUT_JSON          = "ObliQA_MultiPassage.json"
OUTPUT_JSONL        = "ObliQA_Validated_MultiPassage.jsonl"
OUTPUT_JSON         = "ObliQA_Validated_MultiPassage.json"
CACHE_JSON          = "ObliQA_Validation_Cache.json"
BATCH_PROGRESS_JSON = "Batch_Progress.json"
MODEL               = "gpt-4.1-2025-04-14"
FLUSH_EVERY         = 5
SLEEP_RANGE         = (1, 3)  # seconds between requests
SAMPLE_SIZE         = None   # for testing, set to an integer
```
Running the Validator
`python validate.py`

-   It will skip any `(QuestionID, PassageID)` already in the cache.
    
-   Progress is written to `Batch_Progress.json`.
    
-   Final validated data appears in `ObliQA_Validated_MultiPassage.json`.

Statistics & Splitting

After validation, two helper scripts/notebook cells provide:

1.  **Dataset statistics**
    
    `python stats.py` 
    
    – Prints distribution of passages per question, total questions, total passages.
    
2.  **Train/Val/Test split**
    
    `python split.py` 
    
    – By default, splits 70% train / 15% val / 15% test.  
    – Outputs:
    
    -   `ObliQA_MultiPassage_train.json`
        
    -   `ObliQA_MultiPassage_val.json`
        
    -   `ObliQA_MultiPassage_test.json`
        

----------

## Directory Structure

```pgsql

`ObliQA-MultiPassage/
├── ObliQA_MultiPassage.json              # filtered multi-passage questions
├── validate.py (or notebook)             # runs the GPT-4 validation loop ├── ObliQA_Validation_Cache.json          # cached GPT responses
├── ObliQA_Validated_MultiPassage.jsonl   # line-delimited validated output
├── ObliQA_Validated_MultiPassage.json    # flushed JSON  version ├── Batch_Progress.json                   # progress tracker
├── stats.py (or notebook cell)           # prints dataset stats
├── split.py (or notebook cell)           # generates train/val/test splits
├── ObliQA_MultiPassage_{train,val,test}.json
└── README.md`
```
## Contributing

Feel free to submit issues or pull requests. If you extend the validation logic or add new splits, please update the corresponding scripts and README.

